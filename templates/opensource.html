{% extends "main.html" %}
{% block title %}Page Title{% endblock %}
{% block content %}
<h1>Python Packages</h1>
    
<h2><a href="https://github.com/Mukullight/nullvalue">Null Val</a></h2>

<div class="project-preview">
    <a href="https://github.com/mukullight" target="_blank">

    <!-- paste the bootstraped image here  -->
    <div class="col-md-4">
     <img src="../static/images/projects/nullval.png" alt="NULLVAL" class="mukul-project-image">
    </div>
    </a>
    <div class="project-info">
        <h2>NULLVAL</h2>
        <p>A Library for data cleaning</p>
        <p>
            * This repository contains different kinds of methods for the treatment of null values and outliers.
            <br>
            * Using various kinds of numerical techniques for the ideal replacement of values in your dataframe.
        </p>
    </div>
</div>

<blockquote class="twitter-tweet" data-media-max-width="560">
    <p lang="zxx" dir="ltr">
        <a href="https://t.co/4WbiTEjiMU">pic.twitter.com/4WbiTEjiMU</a>
    </p>
    &mdash; Mukul Namagiri (@MNamagiri66052) 
    <a href="https://twitter.com/MNamagiri66052/status/1882516189378994420?ref_src=twsrc%5Etfw">January 23, 2025</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>The project source code is available on the official repository: <a href="https://github.com/Mukullight/nullvalue">GitHub</a>.</p>

<h3>I contribute as developer/maintainer of the following Python packages:</h3>


<button class="btn btn-danger mb-3" onclick="window.open('https://factity.co.uk/agent.html', '_blank')">Link for the below blog -></button>
    

    
<iframe src="https://factity.co.uk/agent.html" frameborder="0" allowfullscreen class="w-100" style="height: 60vh;"></iframe>


<div class="container my-5">
        <div class="row justify-content-center">
            <div class="col-lg-10">
                <h1 class="text-center mb-4 pb-3 border-bottom border-secondary-subtle">Forecasting Photovoltaic Output: A Data-Driven Approach to Solar Energy Optimization</h1>
                
                <p class="lead">In the pursuit of sustainable energy solutions, accurate prediction of photovoltaic (PV) output remains a critical challenge. Solar power generation is inherently variable, influenced by a complex interplay of meteorological factors. This analysis explores a machine learning framework designed to forecast daily PV energy yield from environmental data, drawing on a dataset of 793 observations spanning late 2019. By integrating classical regression techniques with a deep feedforward neural network, the model achieves high predictive accuracy, offering insights for grid integration and energy planning.</p>

                <h2 class="mt-5 mb-3">Understanding the Variables and Their Interdependencies</h2>
                
                <p>The dataset encompasses key solar and environmental metrics, each bearing distinct implications for PV performance:</p>
                
                <ul class="list-group list-group-flush">
                    <li class="list-group-item"><strong>Solar Radiation (W/m²)</strong>: Measures instantaneous solar irradiance, serving as the primary driver of energy capture.</li>
                    <li class="list-group-item"><strong>Solar Energy (MJ/m²)</strong>: Represents integrated radiation over a day, directly scaling with potential PV generation.</li>
                    <li class="list-group-item"><strong>PV Output (kWh)</strong>: The target variable, quantifying electricity produced by panels, modulated by radiation, efficiency, and area.</li>
                    <li class="list-group-item">Auxiliary factors: Precipitation (mm), cloud cover (%), humidity (%), atmospheric pressure (millibars), UV index, and minimum/maximum temperatures (°C).</li>
                </ul>
                
                <p class="mt-3">These elements interact systematically. For instance, elevated solar radiation correlates strongly with increased PV output (Pearson r ≈ 0.93), yet cloud cover attenuates irradiance by up to 100%, while temperatures exceeding 25°C can reduce panel efficiency by 0.4–0.5% per degree due to thermal losses. Precipitation, meanwhile, may cleanse panels but often coincides with reduced insolation. Such relationships underscore the need for robust modeling to disentangle direct and indirect effects.</p>

                <h2 class="mt-5 mb-3">Data Preparation: Ensuring Analytical Integrity</h2>
                
                <p>Initial examination revealed structural inconsistencies: 34 missing values (primarily in solar radiation) and a mislabeled date column. Employing Pandas for manipulation, the process involved:</p>
                
                <ol class="list-group list-group-numbered">
                    <li class="list-group-item">Renaming and parsing dates to datetime format.</li>
                    <li class="list-group-item">Imputing nulls via a localized mean (averaging up to three preceding and succeeding observations), preserving temporal continuity.</li>
                    <li class="list-group-item">Transforming skewed distributions—precipitation exhibited positive skewness (2.88)—through log-normalization, followed by Min-Max scaling to [0,1].</li>
                </ol>
                
                <p class="mt-3">Exploratory analysis, facilitated by Seaborn pair plots and Y-Data Profiling, illuminated distributions and correlations. Kurtosis values ranged from -1.22 (PV output) to 10.24 (precipitation), signaling leptokurtic tails in precipitation data. Categorical treatment of the UV index (11 discrete levels) further refined visualizations, including bar charts and box plots, which highlighted outliers and multivariate dependencies.</p>
                
                <p>The resultant normalized dataset, archived as a CSV, comprised 10 features (excluding date), ready for supervised learning.</p>

                <h2 class="mt-5 mb-3">Classical Regression Models: Establishing Baselines</h2>
                
                <p>An ensemble of regression algorithms was evaluated on an 80/20 train-test split, prioritizing mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R². Results, summarized below, favor tree-based methods for handling non-linearities:</p>
                
                <div class="table-responsive">
                    <table class="table table-striped table-hover">
                        <thead class="table-dark">
                            <tr>
                                <th>Model</th>
                                <th>RMSE</th>
                                <th>R²</th>
                                <th>MAE</th>
                                <th>Training Time (s)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Linear Regression</td>
                                <td>0.100</td>
                                <td>0.863</td>
                                <td>0.077</td>
                                <td>0.004</td>
                            </tr>
                            <tr>
                                <td>Ridge</td>
                                <td>0.101</td>
                                <td>0.862</td>
                                <td>0.078</td>
                                <td>0.002</td>
                            </tr>
                            <tr>
                                <td>Lasso</td>
                                <td>0.272</td>
                                <td>-0.012</td>
                                <td>0.242</td>
                                <td>0.002</td>
                            </tr>
                            <tr>
                                <td>Elastic Net</td>
                                <td>0.272</td>
                                <td>-0.012</td>
                                <td>0.242</td>
                                <td>0.002</td>
                            </tr>
                            <tr>
                                <td>SVR</td>
                                <td>0.118</td>
                                <td>0.809</td>
                                <td>0.098</td>
                                <td>0.005</td>
                            </tr>
                            <tr>
                                <td>Decision Tree</td>
                                <td>0.115</td>
                                <td>0.820</td>
                                <td>0.087</td>
                                <td>0.005</td>
                            </tr>
                            <tr class="table-success">
                                <td>Random Forest</td>
                                <td>0.092</td>
                                <td><strong>0.884</strong></td>
                                <td>0.070</td>
                                <td>0.336</td>
                            </tr>
                            <tr class="table-info">
                                <td>Gradient Boosting</td>
                                <td><strong>0.093</strong></td>
                                <td>0.882</td>
                                <td>0.071</td>
                                <td>0.201</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <p class="mt-3">Random Forest emerged as the superior baseline, with an R² of 0.884 indicating 88.4% explained variance. Residual diagnostics confirmed model adequacy: scatter plots exhibited homoscedasticity, with deviations clustered near zero.</p>

                <h2 class="mt-5 mb-3">Advanced Modeling: A Deep Feedforward Neural Network</h2>
                
                <p>To capture hierarchical interactions, a Deep Fully Connected Neural Network (DeepFCNN) was implemented in PyTorch, featuring 11,997 parameters across four linear layers (9 → 128 → 64 → 32 → 1). Architectural components include:</p>
                
                <ul class="list-group list-group-flush">
                    <li class="list-group-item">Batch normalization after each linear layer to mitigate internal covariate shift.</li>
                    <li class="list-group-item">ReLU activations for non-linearity.</li>
                    <li class="list-group-item">Dropout (0.3 in initial layers, 0.2 terminally) to curb overfitting.</li>
                </ul>
                
                <p class="mt-3">Standardized inputs underwent 1,000 epochs of Adam optimization (learning rate 0.0001) under MSE loss. Convergence yielded test losses below 0.07, surpassing classical models with RMSE 0.085 and R² 0.901. This enhancement reflects the network's proficiency in modeling subtle gradients, such as temperature-mediated efficiency declines.</p>

                <h2 class="mt-5 mb-3">Implications and Future Directions</h2>
                
                <p>This framework demonstrates that PV output can be forecasted with over 90% accuracy using accessible environmental data, bolstering renewable integration amid climate variability. Ensemble regressors provide efficient baselines, while neural architectures excel in feature interplay.</p>
                
                <p>Prospective extensions include temporal modeling (e.g., LSTMs for seasonality) and deployment via APIs for real-time applications. As solar adoption accelerates, such predictive tools will be indispensable for energy resilience.</p>

                <p class="published">Published October 26, 2025 | Data Science & Sustainability</p>
            </div>
                <iframe src="../static/images/1739804930091.pdf" width="90%" height="1000px" allow="autoplay"></iframe>
        </div>
    </div>

<div class="container my-5">
    <div class="row justify-content-center">
        <div class="col-lg-10">
            <h1 class="text-center mb-4 pb-3 border-bottom border-secondary-subtle">Discovering Criteria for Ranking Aspects in Reward Modeling</h1>
            
            <p class="lead">With the increased latency levels in modern large language models (LLMs) and the growing use of techniques like reward modeling, Reinforcement Learning-based approaches have become more popular for fine-tuning LLM responses. This paper introduces a new method that employs aspect extraction to identify specific criteria within generated candidate responses, evaluating them based on positive or negative aspects to reduce the time needed for human preference labeling. An automated pipeline is presented for efficient response assessment, minimizing downtime in the preference optimization process.</p>

            <h2 class="mt-5 mb-3">Abstract</h2>
            
            <p>With the increased latency levels in modern large language models (LLMs) and the growing use of techniques like reward modeling, Reinforcement Learning-based approaches have become more popular for fine-tuning LLM responses. In this paper, we introduce a new method that employs aspect extraction to identify specific criteria within the generated candidate responses. These criteria are then used to evaluate responses based on their positive or negative aspects, reducing the time needed for human preference labeling. Lastly, we present an automated pipeline that evaluators can use to assess responses efficiently, minimizing downtime in the preference optimization process.</p>

            <h2 class="mt-5 mb-3">Introduction</h2>
            
            <p>An AI system that is aligned towards human preference is not just desirable but necessary. The system should be devoid of any hacking strategies and should possess a risk-averse approach in its response generation. The emerging phenomenon of intelligence remains a largely unsolved mystery to say the least. The so-called intelligence and linguistic provenance in these models come from the statistical patterns on which these models are trained rather than the explicit logical inference their human counterparts seem to show. Humans most often tend to get it from exposing themselves to different environments and situations but it is physically impossible for these models. Hence alignment of these models is far more challenging task.</p>
            
            <p>The language models of the time exhibit several remarkable skills from coding, reasoning, and textual generation to going beyond natural language processing tasks. These abilities of the llms have generalized to solve complex real-world problems like autonomous agent control to robotics have made the front and center of the conversation of the AI. With the recent advancements like chain-of-thought reasoning, reflective reasoning, tree search etc have made these llms from simple auto regressive machines that simply predict the next token to cognitive machines with the ability to reason in several stages and produce the most accurate response.</p>
            
            <p>Despite their remarkable capabilities llms are prone to significant shortcomings in maintaining context relevance and logical consistency for longer periods although some of the pre training methods by tuning the architectures of the models have solved this problem to some extent like capturing contextual interdependency in tasks like question answering and maintaining generative coherence in producing tokens but for enhancing the practical utility of these models post training is the way to go. The period from the later parts of the 2020s have seen significant strides in the utility of RLHF for fine tuning the responses. The post training phase of the language models have diversified to address the problems of domain specificity, ethical robustness, integration of multi modality etc recent years which led to the development of techniques like RAG (Retrieval augmented generation), different variants of preference optimization and increased adoption of techniques like mixture of experts etc. By 2024 traditional fine tuning techniques has evolved into RL based strategies.</p>

            <p class="published">Published July 19, 2025 | Author: Mukul Namagiri</p>
        </div>
    </div>
</div>





         <!--/home/mukullight/Music/new/flaskport/static/images/1739804930091.pdf-->
{% endblock %}